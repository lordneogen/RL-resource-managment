behaviors:
  Test_main:  
    trainer_type: ppo  
    hyperparameters:
      batch_size: 128  # Количество шагов опыта на один шаг оптимизации
      buffer_size: 2048  # Количество шагов, сохраняемых для обновления сети
      learning_rate: 3.0e-4  # Скорость обучения — начните с этого значения, затем можно уменьшить до 1e-4 или 5e-5
      beta: 5.0e-4  # Регуляризация энтропии, которая помогает разнообразию действий
      epsilon: 0.2  # Коэффициент, регулирующий степень, до которой действия могут меняться
      lambd: 0.95  # Фактор усреднения для общего градиента
      num_epoch: 3  # Количество эпох для каждого обновления сети
    network_settings:
      normalize: true  # Нормализация входных данных
      hidden_units: 256  # Количество нейронов в каждом скрытом слое
      num_layers: 2  # Количество скрытых слоев; начните с двух, можно увеличить до трех для более сложного поведения
    reward_signals:
      extrinsic:
        gamma: 0.99  # Дисконтирующий фактор для будущих вознаграждений
        strength: 1.0  # Вес внешнего вознаграждения
    max_steps: 1.0e6  # Общее количество шагов, после которого обучение останавливается
    time_horizon: 64  # Длина временного горизонта, после которого обрезается последовательность опыта
    summary_freq: 100  # Частота записи статистики в TensorBoard
